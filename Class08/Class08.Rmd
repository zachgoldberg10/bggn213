---
title: "Class08"
author: "Zach Goldberg"
date: "10/25/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means example

Make example data
```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3)) 
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

Clustering
```{r}
km <- kmeans(x, centers = 2, nstart = 20)
```

Inspect/print the results
```{r}
km
```

Q. How many points are in each cluster? 30 in each
Q. What ‘component’ of your result object details
- cluster size? *km$size*
- cluster assignment/membership? *km$cluters* - cluster center *km$center*?

Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points
      
```{r}
cl <- km$cluster
table(cl)
cl[cl==1] <- "blue"
cl[cl==2] <- "red"

plot(x, col = cl, pch =16)
points(km$centers, pch = 1, cex = 2)

```

## Hierarchical Clustering

```{r}
# First we need to calculate point (dis)similarity 
# as the Euclidean distance between observations 
dist_matrix <- dist(x)

# The hclust() function returns a hierarchical # clustering model

hc <- hclust(d = dist_matrix)

# the print method is not so useful here
hc


```

      
```{r}
dist_matrix <- dist(x)

# Our input is a distance matrix from the dist() 
# function. Lets make sure we understand it first

dist_matrix <- dist(x)

dim(dist_matrix)

## View( as.matrix(dist_matrix) )
dim(x)

hc <- hclust(dist(x))

```

We can plot the results as a dendrogram
```{r}
plot(hc)
```

Can 'cut' tree by height
```{r}
plot(hc)
abline(h=6, col="red")
abline(h=4, col = "blue")
cuth6 <- cutree(hc, h=6) # Cut by height h=6
table(cuth6)

cuth4 <- table(cutree(hc, h=4)) # Cut by height h=4
table(cuth4)
```
Can also cut by desired K
```{r}
plot(hc)
cutbyk <- cutree(hc, k = 2)
plot(x, col = cutbyk)
table(cutbyk)
```

My turn
```{r}

# Step 1. Generate some example data for clustering
x <- rbind(
matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1 
matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2 
matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
rnorm(50, mean = 0, sd = 0.3)), ncol = 2)) 
colnames(x) <- c("x", "y")

# Step 2. Plot the data without clustering
plot(x)

# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results) 
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
Q. How does this compare to your known 'col' groups?

```{r}
par(mfrow=c(2,2))

hc <- hclust(dist(x))
plot(hc)

plot(x, col=col, main = "Known clusters")

cut1 <- cutree(hc, k= 2)
table(cut1)
plot(x, col = cut1, main = "HC with k=2")

cut2 <- cutree(hc, k = 3)
table(cut2, col) #cross tablulate
plot(x, col = cut2, main = "HC with k=3" )
```

## PCA

Download and read in data
```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
row.names=1) #row.names = 1 removes the weird X thing for the first column
head(mydata) #quick overview
dim(mydata) #rows and columns

```
Call prcomp
```{r}
pca <- prcomp(t(mydata), scale=TRUE)
attributes(pca)
```

The returned pca$x here contains the principal components (PCs) for drawing our first graph.
Here we will take the first two columns in pca$x (corresponding to PC1 and PC2) to draw a 2-D plot

```{r}

head(pca$x)
plot(pca$x[,1], pca$x[,2])
```

```{r}
summary(pca)
```
Scree plot
```{r}
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab=)
```
A pretty plot
```{r}
## A vector of colors for wt and ko samples
colvec <- as.factor( substr( colnames(mydata), 1, 2) )
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
xlab=paste0("PC1 (", pca.var.per[1], "%)"), ylab=paste0("PC2 (", pca.var.per[2], "%)"))
identify(pca$x[,1], pca$x[,2], labels=colnames(mydata))
```

## Hands on Example with UK foods

```{r}
x <- read.csv("https://bioboot.github.io/bggn213_f17/class-material/UK_foods.csv", row.names = 1)
dim(x)
head(x)
```
Explore
```{r}
par(mfrow = c(1,2))
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
pairs(x, col=rainbow(10), pch=16)
```

PCA to the rescue
```{r}
pca <- prcomp(t(x))
summary(pca)
```
Plot PCA

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500), col = c("red", "green", "blue", "orange"))
text(pca$x[,1], pca$x[,2], colnames(x), col = c("red", "green", "blue", "orange"))

```

Below we can use the square of pca$sdev , which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for.
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

or the second row here...
```{r}
z <- summary(pca)
z$importance
```

This information can be summarized in a plot of the variances (eigenvalues) with respect to the principal component number (eigenvector number), which is given below.
```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

Lets focus on PC1 as it accounts for > 90% of variance
```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

